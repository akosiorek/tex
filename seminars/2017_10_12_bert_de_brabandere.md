# Bert de Brabendere:
he doens't open his mouth while speaking, how fun

## DFNs
Motivation: cases where output is a slightly transformed version of the input.
* Filters should be able to change on the fly, conditioned on the current input
* Generate a filter bank from an input by a neural net
  * e.g. generate a sharepning filter to remove blur to improve classification later on
  * steerable filters

Local filtering - convolution with unshared weights
* huge amount of parameters with a standard neural network; requires lots of data and a huge risk of overfitting
* a filter-generating network can be conv and therefore save lots of parameters, while still generates params for local filters, cool
  * useful for video prediction: filter generated by an RNN conditioned on video frames; filters are soft-maxed to implement shift-by-optiacal-flow behaviour
* visualise the dynamically-generated local filters to get an idea what's happening: learn optical-flow while learning to predict next video frame


## Instance-level segmentation by learning pixel embeddings
It's hard to segment overlapping object. The taken approach computes pixel embeddings (a separate one for each pixel) such that pixels belonging to the same object lie close to each other in the embedding space and far from embeddings belonging to other pixels.  
* Actual segmentation is done by clustering the pixel embeddings
* An embedding is computed per-pixel but is a function of some receptive field centerd on that pixel, not only of that pixel
* Hinge loss for learning the embedding: **why?**
  * when the loss reaches zero, it it possible to cluster perfectly in the embedding space: **why is that true?**
    * because there is a marginal separating classes and class-blobs are spherical by construction, as it's an L2 version of the hinge loss

### clustering
The clustering algorithm is really well fit with the loss function; they haven't tried any other algos.
* compute embeddings with the neural net
* pick a random pixel embedding: as in a point in the embedding space!
* take pixels within the threshold radius and add them to the same class
* do mean-shift
* go back to step 2) but select a pixel that is not yet labelled


### Semantic instance-level segmentation on Cityscapes:
* train a model for semantic segmentation
* train a separate model for instance-level segmentation for every class separately
* that way every instance-level model has to differentiate only objects of a single class, e.g. cars, which should be simpler (no proof, though)

### Remarks
* Great evaluation and comparision with baseline, plus a thorough underlying net architecture search and percision/speed tradeoffs; plus it shows that it's robust to hyperparams, well done
