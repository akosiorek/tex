\section{Submitted Work}
\label{sec:done}

    During my first year as a DPhil student we developed the Hierarchical Attentive Recurrent Tracking (HART) framework, which was submitted to NIPS 2017. 
    This RNN-based model learns to track objects in videos by focusing on small image regions. It does so by using a differentiable attention mechanism, which can effectively crop a part of the image, thereby quickly removing irrelevant parts of the input.
    Upscaling HART to a challenging real-world dataset proved difficult, as end-to-end training on a randomly initialised neural network was very unstable and converged to poor results. 
    To address this issue, we resorted to transfer learning and used AlexNet \citep{Krizhevsky2012} as a feature extractor, which has stabilised the training and improved performance (\emph{cf.} section 5.2. in the paper).
    
    The task of object tracking is fully-supervised, but can be seen as a reinforcement learning problem \citep{Zhang2017a} with a continuous action space, where a policy chooses a bounding-box update at every time-step; the agent receives a reward either at every time-step or at the end of the episode and the reward structure can be chosen based on the distance between the ground-truth bounding-box and the model estimate. In this setup, HART can be seen as a model-free policy. Instead of using a pre-trained feature extractor, it would be possible to utilise a model of the environment to perform off-line training of the policy, similarly to the Dyna framework. If the model is structured and provides correct position estimates of the object, this approach could increase performance of the tracking framework via unlimited model-based data augmentation.
    
    Alternatively, if a generative latent-variable model of image sequences is available, HART could use the latent representation as extracted features, without the need to rely on a feature extractor pre-trained on static image analysis. Even though static image analysis has different characteristics than sequential analysis (\eg data redundancy at consecutive time-steps), image classification models are often used for processing of video (see \eg \cite{Ning2016a}). 
%    Feature extractors pre-trained on static image analysis tasks are often used for processing video sequences (see \eg \cite{Ning2016a}). 
    This approach, while effective, has little justification in neuroscience. 
    In contrary, there is a growing body of evidence indicating the importance of temporal connections in the human visual cortex \citep{Ungerleider2000}, which suggests that the temporal integration of information is vital for building up high resolution representation of the world, and is also confirmed by the empirical results of predictive coding approaches, \emph{cf.} \cref{sec:seq_model}.
    
%    Modern single-object-tracking approaches are based on either metric learning or bounding box regression (\cite{Bertinetto2016} and \cite{Held2016}, respectively). Not only do they need to rely on heuristics (non-differentiable image cropping, explicit scale search) to achieve computational efficiency and accuracy, but they are also fully dependent on a single error signal for learning.
%    HART, on the other hand, exploits the model structure to make more efficient use of the error signal. It uses the ground-truth bounding boxes to derive three related but distinct learning signals, one of each for the model parts. One of them, the object masks for foreground-background segmentation of extracted attention glimpses, can be seen as self-supervision.
%    It serves different purposes: (a) it forces the model to store object appearance information in the hidden state, (b) it encourages better spatial attention prediction, as computing the object mask is easier (lower relative penalty for any mistakes) if the object covers a bigger part of the attention glimpse and (c) since the ground-truth object mask is computed on the fly, it serves as data-augmentation, in the sense that the errors and the learning signal is dependent on the model parameters and is different in every iteration of training even if the input data is the same.
%    Despite being trained under full supervision, HART was a test bench I used to learn about learning in the presence of temporal dependencies and to experiment  with different structures of the objective function so as to maximise learning from a limited amount of data.
    
    Our work on HART resulted in a biologically-inspired algorithm, which advanced the state-of-the-art performance in attentive recurrent tracking. Contrary to modern trackers, it does not use heuristics to update the scale estimate of the tracked object or to choose the search region in the new frame \citep{Bertinetto2016,Held2016}. It is efficient thanks to the attention mechanism and end-to-end trainable. Finally, it has taught us about learning in the presence of temporal dependencies and structured modelling. 
    
%   \includepdf[pages=-, pagecommand={}]{HART_final.pdf}