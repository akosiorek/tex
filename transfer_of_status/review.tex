\section{Related Work}
\label{sec:lit}

%    Things to write about:
%    \begin{itemize}
%        \item Sequence prediction: text, motion, videos
%        \item Predictive Coding
%        \item Semi-supervised learning: learning by association and ladder networks
%        \item behaviour learning by RL: maze navigation, locomotion patters
%        \item unsupervised learning: AIR
%    \end{itemize}


%
%   Unsupervised & Self-supervised Learning
%   
    \subsection{Unsupervised Learning via Generative Modelling}
       While data in general is abundant and cheap, data for supervised learning is often expensive and time-consuming to gather. The majority of ML algorithms require relatively large amounts of labelled training data. One of the explanation states that they start learning without any prior knowledge of the world \addref. This is in stark contrast to humans, who not only have a vast amount of knowledge about the world, but also expand it continuously and without any supervision \citep{Friston2009guide}. One alternative is to perform generative modelling of the probability distribution $\p{\bx} = \int \p{\bx, \bz} \dint \bz$ of observations $\bx$ in terms of some latent variables $\bz$. The latent variables \emph{explain} the observations and can make the joint distribution $\p{\bx, \bz}$ tractable even in the case of an intractable marginal distribution. The latent encoding can be used in downstream tasks\eg for transfer or semi-supervised learning \citep{Pan2010}. \cite{Hinton2006dbn} introduced Deep Belief Networks (DBN) which explain the observations in terms of Bernoulli latent variables. Alternatively, we can approximate the true data distribution by deriving the evidence lower bound (ELBO) on the log probability of the data, which results in variational autoencoders (VAE) \citep{Kingma2013,Rezende2014}. VAEs are much more flexible than DBNs as they allow latent variables from arbitrary probability distribution functions (pdf) and can be trained end-to-end with off-the-shelf gradient-based methods. These approaches are primarily suited to modelling datasets of independent and identically distributed (\emph{i.i.d.}) points.
%
%   Temporal dependencies in data; neural nets can learn dynamics and features from data
%
    \subsection{Sequence Modelling}
        Traditional approaches to sequence modelling often consider inference of latent variables that explain the data\eg linear dynamical systems or hidden markov models \citep{Bishop2006}.
        They often require dynamics of the system to be known and often have too little capacity to model complex and high-dimensional real-world data.
        Neural networks, on the other hand, can learn both features and state dynamics from data and they can approximate functions of arbitrary complexity with arbitrary precision.
        Even early works on the topic demonstrated how useful neural networks are for prediction of chaotic time-series \citep{Lapedes1988}.
        Since then, neural networks have been successfully applied for sequence classification and prediction in different domains: written natural language, speech and audio, motion capture data or brain waves \citep{Langkvist2014}. 
%
%   Sequence Prediction as a type of unsupervised learning
%
        Unsupervised learning can be also done as sequence prediction, where the task is to predict the observation at time $t+1$ given a sequence of observations $\bxts$ up to time $t$. This task is flexible in that it admits many different model types, including Gaussian processes, support vector machines or feed-forward neural networks, although models which can explicitly use temporal structure of data such as Gaussian process dynamic models (GPDM; \cite{Wang2008}) or recurrent neural networks (RNN) tend to perform better. Recently, sequential counterparts of VAEs have been proposed, which allow efficient generative modelling of sequences \citep{Fabius2015,Bayer2015,Karl2017}.
%
%   Predictive Coding
%
    \subsection{Predictive Coding}
    \label{sec:pred_coding}
        Modern sequential predictive models tend to update its hidden state at every time-step. It can be argued, however, that if a model is able to predict the world perfectly, it should not update its state. On the contrary, if a perfect prediction is available, the model should be capable of evolving its hidden state so as to reflect the change of the world \wrt the prediction. Predictive coding formalises this behaviour by using only the prediction errors to update the hidden state. The idea dates back at least to the Kalman filter \citep{Kalman1960}. Recent advances in neural networks allow to frame it as an auto-regressive neural network \citep{Lotter2016, Canziani2017}, which uses the difference between the prediction and the input at the following time step to update the hidden state. 
        As there are many futures possible, this approach leads to an ill-posed problem. The prediction, which is a maximum-likelihood estimate of what might happen, is only a single instantiation thereof. It would be theoretically more sound to normalise prediction errors by the covariance matrix of errors, an approach adopted by Kalman filtering. \cite{Friston2009guide} argues that 
        the human brain might also follow this approach, whereby the computational architecture of the brain forms a hierarchical system, whose every layer constantly tries to predict the output of the lower levels of the hierarchy in a fully Bayesian fashion. The normalised predictive error in this setup gives rise to surprise, which is the negative log-likelihood of the inputs under the predictive distribution of the model and where the normalisation can be understood as an attention mechanism.
%
% AIR, Learning by Association, locomotion behaviours, interaction networks, physics?
%
    \subsection{Learning of Abstract Ideas}
        The utility of sequence prediction as an unsupervised learning approach can be intuitively explained by the fact that predicting the future, if it is to be done well, requires very good understanding of the present. If, for example, a model can learn an idea of an object and the laws of physics, it should be able to constrain its prediction to those physically plausible: \eg a car should not dissolve into thin air. The majority of neural models are over-parametrised \citep{Denil2013}, however, which makes learning abstract notions from data extremely sample inefficient. \cite{Eslami2016} introduce AIR, a VAE with a variable-length latent encoding for image reconstruction. This model imposes a geometric prior on the encoding length which encourages sparse solutions, therefore learning to decompose the scene into a number of independent parts --- the objects. It is worth noting that, along the main model, the authors introduce difference-AIR, which exploits the specific structure of the problem and adheres to the predictive coding paradigm, thereby achieving better performance. In the extension of this work, \cite{Rezende2016} learn to reconstruct three-dimensional (3D) structure of an object from even a single two-dimensional (2D) view by imposing 3D latent representation and structuring the decoder as a projection of the latent space into the 2D output space; they show that their model is able to infer the idea of an object from data.
        \cite{Haeusser2017} learn the idea of an object and its class by learning to associate similar objects with each other in the embedding space, which is very much like a child learning about its identity by comparing itself with others \citep{Decety2003}.
        In case of reinforcement learning, a complex environment might itself be a cue which leads to learning abstract ideas. \cite{Heess2017} shows that articulated agents can learn real-world motion patterns by interacting with the environment. Specifically, they learn to crouch, jump, turn and run while maximising a very simple reward function based on forward progress.
        Using a specific model structure as a method of learning abstract ideas was also demonstrated by \cite{Battaglia2016}. The authors propose an interaction network, a highly complex model that operates on a graph of objects and relations between them and acts as a physics simulator. The particular model structure enables learning invariants (\eg energy conservation) and inferring latent variables describing the system as a whole (\eg potential energy).
        
      
   In the following we put these ideas together.
%	
%	Yet another trait particular to next time-step prediction is that the model maintains some hidden state which describes the state of the world. We can argue that whenever the prediction of the model is perfect (the prediction matches the next time-step perfectly), the hidden state perfectly reflects the state of the world and therefore does not have to be updated. None of the aforementioned approaches make use of that observation, and therefore in every of the presented cases the model has to learn to compute discrepancies between the new observation and the hidden state of the world and to update the hidden state by corrected those discrepancies only. This is possible, as proved by the satisfactory performance of the presented approaches, but that means that all next-timestep prediction settings have a common structure that has been neglected so far. Below I describe the idea of predictive coding, and how frameworks following this approach address the limitation of usual sequence predictors.
%	
%\subsection{Predictive Coding}
%
%	The idea of predictive coding dates back at least to the Kalman filter \addref. Kalman filter is an instance of a Gaussian linear system used for estimating the state of the world. It first computes a prediction of the world state at the next timestep and then, when it gets hold of the observation at that timestep, it updates its prediction. It also estimates noise covariance matrices for the prediction and update steps and their respective contributions are proportional to the inverse covariance matrices. 
%	
%	Friston \addref argues that predictive coding can be implemented as energy minimisation and that energy minimisation can well explain learning in the brain and many neuroscientific phenomona that would otherwise remain quite puzzling to neuroscientists. He suggests that the computational architecture of the brain forms a hierarchical system, where each layer constantly predicts the output of the lower levels of hierarchy in a fully Bayesian fashion. It gives rise to surprise, which is the negative log-likelihood of the inputs under the predictive distribution of the model. Friston argues that surprise leads to error normalisation \wrt prediction uncertainty, which can be interpreted as attention.
%	
%	It is possible to frame the next-timestep prediction in terms of an auto-regressive neural network. Indeed, it has been addressed in a number of papers \citep{Lotter2016, Canziani2017}. These models use the mean-squared error between the prediction and the corresponding input to update their hidden state. It leads to an ill-posed problem since there are many futures possible. The prediction, which is a maximum-likelihood estimate of what might happen, is only a single instantiation thereof.
%    
%\subsection{Semi-supervised and Unsupervised Learning}
%
%    Next time-step prediction and predictive coding both fall in the category of unsupervised learning, where the aim is to learn the probability distribution over sequences of observations $\bx$, namely $\p{\bxTs}$. Some of the approaches do so by assuming latent variables $\bz$ the explain the data and learn the joint distribution of $\p{\bx, \bz}$ instead. Any type of auto-encoder falls within that category.
%    
%    \begin{enumerate}
%        \item autoencoders, denoising, constrastive
%        \item ladder networks
%        \item deep belief nets
%        \item vae
%        \item learning by association
%    \end{enumerate}
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    
%    