\section{INTRODUCTION}

Computers are often praised for being better than humans at things that are inherently hard for humans, yet they are also  criticised for being considerably worse than humans at things that are very easy for humans \addref{where is it?}. While some algorithms perform well without being trained on huge amounts of data \addref{}, they are often domain specific \addref. Often to increase performance, much more complicated model and much larger amounts of data are required. It has been recently shown \addref that performance in computer vision tasks is logarithmic in the size of the dataset. 

Humans, on the other hand, are known to be extremely good at learning. We can often learn and generalise well after seeing only a single labelled data point. Some argue \addref, that this can be explained by learned feature representation. Our brains are hard-wired to build specific representations of the world and are exposed to millions of images every day, constantly inferring truths about the world. That is in contrast to modern machine learning algorithms, which are typically trained only once and they are deployed with their parameters frozen. Moreover, modern algorithms are typically trained from a cold start, without any prior knowledge of the world.

Finally, human cognition often involves using time-dependencies in the data \addref. This contradicts current computer vision practice, where the majority of algorithms is used for inference from single data points. While reasonable in some problem domains, sequential character of data can be leveraged in other domains for higher performance, more consistent model outputs, better learning with better generalization. I would argue that even in domains where inference from single data points is required, training of those algorithms could benefit from sequential nature of data.