\section{Research Proposal}
\label{sec:proposal}


    During the next year, we will develop a series of structured generative models of videos.
    %    We are going to start by building a generative model of a moving object as an extension to the AIR framework.
    Firstly, we are going to leverage recent advances in variational inference and neural networks to build a generative model of moving objects as an extension to the AIR framework. 
    As the problem is hard, we will start with a toy dataset of moving two-dimensional shapes, in order to extend it later to moving three-dimensional shapes in the presence of camera motion.
    %    In the later version I will focus on moving three-dimensional shapes in the presence of camera motion. 
    %    The final step is to improve the AIR model to work on images with rich backgrounds and real-world data, and then extending this modification to the generative models of moving object. 
    Secondly, we are going to improve the AIR model to work on images with rich background and real-world data and then extend this modification to the generative model of moving objects. We expect that this will require a form of object/background segmentation or background subtraction and generative blending of objects and the background.
    Finally, we are going to investigate using trained generative models of videos within the Dyna framework.
    We now detail the above steps.
    
    \subsection{A Generative Model of Moving Objects}
        The generative model of moving objects (GMMO?) extends AIR to track objects by generating them in a sequence in blank canvases. It requires extending AIR to model which can infer appearance and the ratio of its change based on frames 1 to T and then painting that object in frame T+1, with possible looking that frame to infer the position at which the object should be drawn. In this sense, this is an extension to the AIR framework which draws massively from HART
        
    \subsection{Generalisation of the AIR framework}
        
    
    \subsection{A Generative Model of Videos}
    \subsection{Model-based RL}









%Things to write about:
%\begin{itemize}
%	\item Predictive Coding: next-frame prediction with VAEs, it's normalisation behaviour, inner feedback loop for corrections
%	\item Model-based RL: sample efficiency, using a policy to improve learning speed of the perception module, learn a policy in the absence of a goal
%	\item Unsupervised object tracking \& detection
%\end{itemize}

%    Drawing from my experiences with HART, I am going to focus on representation learning for videos. I believe that imposing a specific model structure can lead to better, easier and faster learning and that generative modelling is able to produce neural representations that are easily transferable to other tasks. To this end, I would like to explore the predictive coding paradigm, or rather its instantiation within the variational inference framework. 
%    Going further, I believe it is possible to combine attentive recurrent tracking with an approach similar to AIR \citep{Eslami2016} to create a generative model of a moving object, capable of inferring intuitive physics without any supervision.
%    Finally, I would like to merge these two branches to learn a model of the environment and use it in a model-based reinforcement learning. In the following, I will describe the three ideas, explore connections between them and evaluate associated risks.    
%    
%\subsection{Variational Inference for Predictive Coding}
%\label{sec:pred}
%   
%   Predictive coding describes a family of models for sequence prediction. If a sequence predictor has a hidden state, one can argue that this state should be updated only in case of imperfect predictions, see \cref{sec:seq_mode} for details. \cite{Friston2009guide} argues that this type of sequence modelling is employed in the human brain, where it explains phenomena related to learning. He also represents the view that the brain is Bayesian and that any prediction in the human brain has to be probabilistic. In this case, the model can be optimised by minimising the information-theoretic surprise and the prediction error can be generalised to Mahalanobis distance \wrt the predictive probability distribution. This approach has not been explored in the machine learning literature, and yet it gives rise to a family of models shaped after the VAE, but reformulated for prediction as opposed to reconstruction of the input. This formulation has several advantages, including:
%    \begin{description}
%        \item[Non-stationary priors] A probabilistic prediction of the activations in the latent space can be used as a prior for the latent encoding at the next time-step. It maintains its properties as a regulariser while admitting higher flexibility of the approximate posterior distribution.
%        
%        \item[Self-normalisation] Probabilistic predictive coding can be used for normalisation of activations of neural networks. Given that we minimise surprise as the learning criterion, and assuming Gaussian output probability distribution, we can use the statistics of the distribution to whiten latent encoding at layer $l$ before inputting it to layer $l+1$. It can potentially alleviate or even solve the problem of covariance shift in the encoder part of the model, therefore removing any need for explicit normalisation (\eg batch normalisation). The validity of this argument is supported by the successful usage of neural baselines for variance reduction in score-function estimators \citep{Mnih2014}. It is unclear how normalisation of the encoder will impact learning of the whole system, nor whether it is possible to devise a similar method of normalising the decoder activations.
%    \end{description}
%    While not revolutionary in itself, the predictive coding paradigm imposes structure on the model and constrains the optimisation problem, potentially leading to faster and more sample-efficient learning.
%
%\subsection{Unsupervised Learning to Track Objects}
%\label{sec:obj}
%
%    One can define an object in the image space as a patch of an image, where the correlation between pixels within the patch is strong, while the correlation between pixels inside and outside of the patch is weak. This definition, together with the penalty on the encoding length in the latent space, is in fact what makes AIR work. We can extend this definition to video sequences, where correlation between pixels in patches representing the same object at consecutive time-steps should be high under the assumption of high-enough frame rate. If video frames contain only simple objects on plain background, we can reformulate AIR for frame prediction instead of reconstruction to form a generative model of moving objects. This approach is unlikely to work with rich backgrounds or when an object constitutes only a small part of the scene, however. I believe that these issues can be addressed by background subtraction and soft visual attention, respectively. 
%    
%    \subsubsection{Background Subtraction}
%    
%        Assuming that the appearance of the object is known and is given by a vector $\bvt$, it is possible to segment it out of the image by using a dynamic filter network (DFN; \cite{Brabandere2016dfn}) in a very similar fashion to the dorsal stream of HART. Moreover, this segmentation model can be easily pre-trained (as proved by unpublished preliminary experiments) in an unsupervised way by cropping two overlapping patches from an image, treating one of them as the object and trying to find it within the second patch. If we assume that the initial tight bounding box is available (ground-truth, provided by an external object detector or AIR), we can easily extract the appearance vector.
%    
%    \subsubsection{Visual Attention}
%        If the object is small relative to the image, background subtraction is unlikely to work due to the potentially large amount of noise in the segmentation. We can address this issue by using soft visual attention to crop the object or a small area around it from the image. This approach requires attention parameters, which can be initialised from the bounding box for the first frame or inferred from the sequence seen so far. 
%  
%    Given an attention glimpse $\bgt$ at time $t$ and an appearance vector $\bvt$, we can create an object mask $\bmt$ and compute a masked version of the glimpse $\bgt^m = \bmt \odot \bgt$, where $\odot$ denotes the Hadamard product. Given two masked glimpses at times $t$ and $t+1$, we can predict $\bg_{t+1}^m$ from $\bgt^m$ by using an AIR-like model.
%    
%    To drive learning, we can minimise the prediction loss of $\bg_{t+1}^m$ while maximising the area of the object mask $\bmt$ in the image space at the same time. The trade-off here is that $\bgt^m$ represents the appearance of the object and only the object, therefore it cannot be used to predict anything but the object at the next time-step; therefore minimising the prediction loss will also minimise the positive area of the object mask. Maximising the object mask area will prevent it from shrinking to zero. It will also, together with minimisation of the prediction loss, encourage accurate prediction of attention parameters, since if the attention glimpse does not contain the object it is virtually impossible to predict anything in or segment that glimpse.
%    
%    By combining all of the above, we arrive at a generative model of a moving object, which includes the motion model as well as the appearance model, both conditioned on the image background. Since the model is generative, we can condition it on a short video sequence and generate multiple trajectories in the image space by sampling from the prior; we can therefore examine the model for visual fidelity as well as physical plausibility of the generated paths. One caveat here is that the model will generate only the moving object, without its background. It might be necessary to use an additional component for background prediction, possibly conditioned on the predicted objects.
%
%\subsection{Model-based Reinforcement Learning}
%
%    Predicting the next time-step in a structured manner might prove to be an effective way of representation learning. If a model learns intuitive physics directly from data, it can be useful model-based reinforcement learning. It remains to ask whether we can make the learning of the model faster, more general or more efficient by coupling it with an agent which can interact with its environment. Specifically, I would like to investigate the following:
%    \begin{enumerate}
%        \item Is it possible to use a policy for surprise-maximisation in a predictive coding setting? Can it lead to faster learning by exposing the model to otherwise rare events? 
%        \item Can a surprise-minimisation policy be used in the absence of any explicit goal? How to avoid degenerate solutions in this case?
%        \item Does predicting the next time-step lead to faster learning in RL, especially when rewards are sparse? Does it encourage or discourage exploration?
%    \end{enumerate} 
%    Approaches described in \cref{sec:obj,sec:pred} can be used for model learning in a model-based reinforcement learning setting. They can be used for pretraining or as unsupervised auxiliary tasks. While interactions between proposed methods and reinforcement learning remain unknown, they can potentially improve sample efficiency and scalability of reinforcement learning approaches and I am interested in exploring this topic.
