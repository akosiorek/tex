\section{Research Proposal}
\label{sec:proposal}

%\begin{enumerate}
%    
%    \item Is high-accuracy prediction of the whole image necessary for model-based RL, or are some parts more important?
%    
%    \item Can we build a non-parametric unsupervised latent-variable model of the scene, where latent variables explain objects and their poses and where the encoding length would depend on the number of objects in the scene (hence non-parametric)?
%
%    \item Is it possible to perform prediction or a model-based simulation in the latent space, so as to circumvent deteriorating prediction quality, often visible in the image space? 
%    
%    \item Is it possible to use such latent-space simulations within the Dyna framework, especially with a pre-trained model of the environment dynamics?
%    
%    \item Does scene decomposition require to consider intra-object relations explicitly or is implicit treatment sufficient?
%\end{enumerate}
    
    During the remainder of this DPhil, we will focus on answering the following question: How accurate does a model of the environment have to be in order to be useful in the context of model-based RL? Specifically, we are interested in finding out 
    (i) whether it is possible to identify \emph{important} parts of the scene without any supervision, 
    (ii) if simulating their dynamics separately and explicitly brings about any benefits and 
    (iii) how relations between them affect the simulation.
    To this end, we are going to develop a structured non-parametric generative latent-variable model of the environment dynamics.
    By imposing a specific structure within the VAE framework, we can enforce scene decomposition by putting an \emph{appropriate} prior on the latent space, similar to AIR, and thereby gaining insight into what the model deems to be important.
    Making the model non-parametric allows us to work with a variable number of objects, which allows tackling scenes and tasks of varying complexity.
    It also enables us to simulate objects separately and analyse relations between them, thereby answering question (ii) and (iii).
    Using a generative model is required by model-based RL, while at the same time providing insights into its internal workings: we can qualitatively evaluate samples generated by the model. 
    To validate our endeavours, we are going to employ our generative model in the Dyna framework and use it in simulation in partially-observable environments involving robotics and/or computer games. We now detail the above.

    \subsection{Generalisation of the Attend, Infer, Repeat framework}
        The AIR framework can be seen as a non-parametric latent-variable generative model of images, which is very close to what we would like to develop.
        It reconstructs an image by detecting objects present therein without supervision and paiting one object at a time in a blank canvas.
        The original work showed it to work only in the context of plain backgrounds (black, in fact) and simple objects.
        In order for the model to be useful in any real-world setting, it has to be able to handle images with rich backgrounds and occluded and complex objects. 
        We expect that this will require a form of object/background segmentation or background subtraction and generative blending of objects and the background.  
        Given that AIR uses a spatial transformer \citep{Jaderberg2015} to draw objects in a canvas, it is straightforward to create an explanation mask, which marks which locations in the canvas has been drawn to. 
        When objects are explained, it should be possible to use the explanation mask with a complementary background model to explain the remainder of the image.
        Separating reconstruction of the background and the objects might create discontinuities at the boundaries, however, and it is unclear how to prevent the background model from explaining the objects at the same time.
        It is our intuition that pixel correlations within objects are different than in the background or between objects and their neighbourhoods.
        If we parametrise background- and object-generating models with a minimum-length encoding scheme, it should force them to learn their problem-specific correlation structure, therefore forcing the parts of the scene to be explained by corresponding model components.
        Since the KL-divergence term in the VAE loss can be interpreted as an information-bottleneck \citep{Achille2016}, VAE effectively minimises encoding-length of the latent representation.
        
        We will start by working with a multi-MNIST dataset, similar to the one used by the original paper, but with a noisy background. 
        The goal is to upscale the approach to real-world images and\eg ImageNet dataset.
        At this point it is unclear whether the approach based on AIR will work. If it does not, we might have to introduce more structure by \eg image segmentation or employ semi-supervised learning instead of a purely unsupervised approach.
        We expect this phase to take between 4 and 6 months and result in an article submitted to either IJCAI or ICML in February 2018.
    
    \subsection{A Generative Dynamical Model of Moving Objects}
        Once we are able to detect salient parts of an image, we are going to focus on modelling environment dynamics in terms of dynamics of individual objects.
        We will focus on developing a transition function in the latent space, so that the reconstruction after the transition will lead to prediction of the scene at the following time-step.
        It will results in a model that is effectively able to track objects by generating them one-at-a-time in a sequence of blank canvases.
        To reconstruct an image, AIR decomposes it into a set of $\bz^{\mathrm{where}}$ and $\bz^{\mathrm{what}}$ latent variables, which describe location and appearance of an object, respectively.
        The sequential model will need to take time-dependencies into account.
        In particular, instead of directly using $\bz^{\mathrm{where}}$ and $\bz^{\mathrm{what}}$ inferred from an image $\bxt$ at time $t$ to reconstruct the image at time $t+1$, it will need to take into account the history of appearances and locations $\bzts$ at times 1 to t.
        This can be accomplished by using a learned dynamics model, \eg an RNN. 
        In practice, the dynamics model should take into account actions executed by the agent, which might lead to transition functions similar to the ones of \cite{Watter2015} and \cite{Karl2017}.
                
        Even though the modification to AIR looks simple, it is unclear whether this approach will work. Firstly. it is based on the assumption (like AIR) that the correlation between pixels within an object is much stronger than correlation between pixels inside and outside of the object. Secondly, this model is not allowed to peek at the image at time $t+1$ to reconstruct it, which severely increases the difficulty of the task. To address this issues, we are going to start simple, with a toy dataset of moving two-dimensional shapes. We will extend it later to moving three-dimensional shapes in the presence of camera motion.
        
        In the absence of data, the model allows simulation by updating the latent state $\bzt$ with samples drawn from a prior distribution $\p{\bz}$.
        Choosing the right prior for a sequential task poses a research question by itself \citep{Soelch2016} and might require significant effort to answer.
        The transition function of the dynamics model is another crucial component of the model. It defines dynamics in the latent space and it will determine whether the model adheres to the laws of physics.
        %
        We expect this stage to take about two to four months and we expect that it will lead to a publication, possibly for NIPS, whose deadline is in May 2018.
       
    
    \subsection{A Generative Model of Videos}
        Combining the generalised version of AIR with a generative model of moving objects will result in a structured generative model of image sequences. While simple in principle, we expect a number of issues to arise. Firstly, the moving object model does not take the background of the target image into account. We might have to modify the model to predict the background and use it to condition the locations of the objects in the target image; alternatively we can condition background generation on the object appearance and their location. 
        Secondly, training a high-fidelity model on video sequences is computationally very expensive due to the huge amounts of data this approach requires.
        Additionally, it is unclear which output probability distribution to use; output probability distribution in VAEs is responsible for the shape of the loss landscape. Gaussian assumption about prediction errors is not well justified when dealing with images and temporal dependencies between model outputs aggravate this issue even further. This issue might require further research (Generative Adversarial Networks might hold an answer; \cite{Wenzhe2016}) if satisfactory performance is to be attained. We expect this stage to take 4 to 6 months.
        We would like to turn it into a publication for ICLR with the deadline in October 2018.
    
    \subsection{Model-based RL}
        With the above components ready, we will be in a position to answer the initial question about reinforcement learning:
        how accurate does the model of the environment have to be?
        We will attempt to use our object-centric dynamic model of the environment in the Dyna framework and compare it to baselines.
        It will allow us to examine how does the non-parametric treatment of parts of the scene affect simulation within the RL context and whether object-centric representation is useful for a model-free policy and whether explicit reasoning about relations between objects is of importance.
        By enforcing handling different parts of the scene by different model components, we will be able to examine how does the representation type of various parts of the scene affect performance of an RL agent. 
        This approach has the potential for improving sample efficiency within the Dyna framework while granting improved interpretability. Given difficulties with training non-linear models of environment, however, it is unclear whether this approach will work. A generative model of videos with variable-length encoding factorised between parts of the scene can be very useful in latent-space control algorithms similar to E2C, especially when any form of relational reasoning is required. In this case, the object-based representation delivered by AIR-like modelling can be married with structured reasoning models such as relational nets of \cite{Santoro2017} or the dynamic neural computer of \cite{Graves2016}.
        We expect the final stage to take the remainder of this DPhil.

